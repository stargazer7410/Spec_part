 Copilot Instructions for New Chat
ğŸ“ Project Name
Spec_part

ğŸ§  Purpose
Automate client-specific data processing using:

Azure Data Factory (ADF) for orchestration
Databricks for execution
Modular Python jobs for logic
Dynamic SQL for querying
Cloud-agnostic observability via Azure Monitor or GCP Cloud Monitoring


ğŸš€ Workflow Summary

ADF triggers Databricks with:

file_directory
filename
cloud_provider (e.g., azure, gcp)


main.py receives these arguments
Loads config from config/clients/{filename}.json
Dynamically imports and runs the job module from python/jobs/{filename}.py
Job module:

Executes SQL from sql/{filename}.sql
Processes rows with error handling
Logs events using cloud-specific logger
Saves failed rows to ADLS or GCP Storage (future enhancement)




ğŸ“ Folder Structure
Spec_part/
â”œâ”€â”€ main.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â””â”€â”€ thyme-care.json
â”‚   â””â”€â”€ job_params.json
â”œâ”€â”€ enhancements/
â”‚   â””â”€â”€ README.txt
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ thyme_care.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ param_loader.py
â”‚       â”œâ”€â”€ error_handler.py
â”‚       â”œâ”€â”€ log_manager.py
â”‚       â”œâ”€â”€ azure_monitor.py
â”‚       â”œâ”€â”€ gcp_monitor.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ thyme-care.sql
â”œâ”€â”€ Instructions/
â”‚   â”œâ”€â”€ project_overview.txt
â”‚   â””â”€â”€ developer_onboarding.txt
â”œâ”€â”€ README.md


âœ… Key Features

Config-driven job execution
Dynamic SQL with parameter substitution
Row-level error handling
Cloud-agnostic logging via log_manager.py
Modular job design for easy extension
Developer documentation and onboarding


ğŸ§ª Runtime Parameters
Passed from ADF to Databricks:
Shellpython main.py <file_directory> <filename> <cloud_provider>Show more lines
Example:
Shellpython main.py /data/clients thyme-care azureShow more lines

ğŸŒ Cloud Provider Setup
Set the environment variable CLOUD_PROVIDER dynamically via Databricks or ADF:

Azure: CLOUD_PROVIDER=azure
GCP: CLOUD_PROVIDER=gcp

This is used by log_manager.py to route logs to the appropriate cloud utility.

ğŸ“˜ Developer Onboarding Summary

Clone repo
Create virtual environment
Install dependencies
Set CLOUD_PROVIDER
Run main.py with required arguments
Add new jobs in python/jobs/ and SQL in sql/
Update config in config/clients/
Project automates client-specific data processing using cloud-agnostic Python jobs triggered by ADF and executed in Databricks.