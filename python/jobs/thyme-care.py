from python.utils import sql_runner
from python.utils.error_handler import log_job_observation
from datetime import datetime

def run(spark, params):
    print("ðŸš€ Running Thyme-Care job")
    start_time = datetime.now()
    df = sql_runner.run_sql_for_job(spark, params)
    source_record_count = df.count()
    success_count = int(source_record_count * 0.98)
    failure_count = source_record_count - success_count
    log_job_observation(
        job_name=params["job_name"],
        job_run_time=start_time,
        source_record_count=source_record_count,
        success_count=success_count,
        failure_count=failure_count
    )
    df.show()
